{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### install necessary packages and libraries"
      ],
      "metadata": {
        "id": "YjMOr-WNkVfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "id": "yGZU10yMkVH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b902b2-f429-4120-8f9c-4b0ef40f5f7e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary packages and libraries"
      ],
      "metadata": {
        "id": "w6jxjaWMj9m6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "NWNqkY4Lj6Yp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the dataset"
      ],
      "metadata": {
        "id": "HbPcjWFsmIN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "dataset_file = '/content/drive/MyDrive/new_updated_dataset.csv'\n",
        "\n",
        "data=pd.read_csv(dataset_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUDQSk3Sl3_l",
        "outputId": "402f91c8-f4f8-4f50-8be2-ebc5ed7de424"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import pre-trained models(fasttext and emoji2Vec)"
      ],
      "metadata": {
        "id": "7NkOi8colfzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import fastText model for text\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "import gensim.models as gsm\n",
        "\n",
        "fastText_model_path = '/content/drive/MyDrive/cc.si.300.bin/cc.si.300.bin'\n",
        "fasttext.FastText.eprint = lambda x: None\n",
        "ft = fasttext.load_model(fastText_model_path)\n",
        "\n",
        "\n",
        "#import emoji2Vec model for emojis\n",
        "\n",
        "e2v = gsm.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/emoji2vec.bin', binary=True)"
      ],
      "metadata": {
        "id": "jYyHFU0YkNC_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Preprocessing data"
      ],
      "metadata": {
        "id": "WImIrfJSsU1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE) # Eliminate links\n",
        "    text = re.sub(r'@\\w+', '', text) # Eliminate mention names\n",
        "    text = re.sub(r'#\\w+', '', text) #Eliminate # signs\n",
        "    text = re.sub(r'$\\w+', '', text) # Eliminate $ sign\n",
        "    text = re.sub(r\"[^à¶…-à·†a-zA-Z\\s]\", '', text)  # Keep Sinhala and English letters only\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # k\n",
        "    return text\n",
        "\n",
        "\n",
        "# Preprocess comments\n",
        "column_name = 'comments'\n",
        "if column_name in data.columns:\n",
        "    data[column_name] = data[column_name].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "aMoBX45WsTyT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle the dataset"
      ],
      "metadata": {
        "id": "rINgHTJcm9i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "data = shuffle(data)\n"
      ],
      "metadata": {
        "id": "ciWCT9Qlm-H6"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset"
      ],
      "metadata": {
        "id": "x5kkIL4WoLkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# combined the emoji and text column for split in same points\n",
        "\n",
        "combined_data = pd.concat([data['comment'], data['expression_emoji']], axis=1)\n",
        "\n",
        "#split the dataset into train, test, validation\n",
        "combined_train, combined_test, y_train, y_test = train_test_split(combined_data, data['label'], test_size=0.2, random_state=42, stratify=data['label'])\n",
        "\n",
        "combined_actual_train,combined_val,y_actual_train,y_val=train_test_split(combined_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "#devide each column under train, test, validation\n",
        "X_actual_train_text, X_actual_train_emoji = combined_actual_train['comment'], combined_actual_train['expression_emoji']\n",
        "\n",
        "X_test_text, X_test_emoji= combined_test['comment'], combined_test['expression_emoji']\n",
        "\n",
        "x_val_text,x_val_emoji=combined_val['comment'],combined_val['expression_emoji']\n"
      ],
      "metadata": {
        "id": "bFvIKiC7FWT7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform word to vector using fastText model for comment column"
      ],
      "metadata": {
        "id": "6_hLp25GnR8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_vector(word):\n",
        "    try:\n",
        "        wordVec=ft.get_word_vector(word)\n",
        "\n",
        "        return wordVec\n",
        "    except KeyError:\n",
        "        return np.zeros(ft.get_dimension())\n",
        "\n",
        "def get_sentence_vector(sentence):\n",
        "\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "\n",
        "    if not sentence.strip():\n",
        "        return np.zeros(ft.get_dimension())\n",
        "\n",
        "    vectors = [get_word_vector(word) for word in sentence.split()]\n",
        "\n",
        "    return np.mean(vectors, axis=0)"
      ],
      "metadata": {
        "id": "Pwl7cVLmnTUJ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_text_vectors=np.array([get_sentence_vector(sentence) for sentence in X_actual_train_text])\n",
        "X_test_text_vectors=np.array([get_sentence_vector(sentence) for sentence in X_test_text])\n",
        "x_text_val_vectors=np.array([get_sentence_vector(sentence) for sentence in x_val_text])"
      ],
      "metadata": {
        "id": "jSKIuYT5pbx5"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " perform Emoji to vector task using emoji2Vec model for expression_emoji column"
      ],
      "metadata": {
        "id": "Ctj3GEOFrTN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(get_word_vector('à¶­à·’à¶ºà·™à¶±à·€à·')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eIGQsmyrYmw",
        "outputId": "932f41b4-4c7d-4169-8e18-eb23973d319f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009083676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(get_sentence_vector('à¶¸à·šà¶šà·™à¶­à·Š à·€à·™à¶±à¶¸à¶¸ à¶†à¶­à¶½à·Š à¶‘à¶šà¶šà·Š à¶­à·’à¶ºà·™à¶±à·€à·!!')))"
      ],
      "metadata": {
        "id": "bv_2bCnCzMKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_emoji_vector(emoji):\n",
        "\n",
        "    try:\n",
        "        value=-e2v[emoji]\n",
        "\n",
        "        return value\n",
        "\n",
        "    except KeyError:\n",
        "        return np.zeros(300)"
      ],
      "metadata": {
        "id": "I-Ut5-b_rUTp"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textHate=np.mean(get_word_vector('à¶´à·œà¶±à·Šà¶±à¶ºà·™à¶šà·Šà¶¯'))\n",
        "print(textHate)\n",
        "emojiHate=np.mean(get_emoji_vector('ğŸ˜¡'))\n",
        "print(emojiHate)"
      ],
      "metadata": {
        "id": "w_beAu6A4H-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textNon=np.mean(get_word_vector('à¶­à·’à¶ºà·™à¶±à·€à·'))\n",
        "print(textNon)\n",
        "emojiNon=np.mean(get_emoji_vector('ğŸ˜‚'))\n",
        "print(emojiNon)"
      ],
      "metadata": {
        "id": "MxLae8NB6rmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textNon=np.mean(get_word_vector('à¶­à·’à¶ºà·™à¶±à·€à·'))\n",
        "print(textNon)\n",
        "emojiNon=np.mean(get_emoji_vector('ğŸ¥°'))\n",
        "print(emojiNon)"
      ],
      "metadata": {
        "id": "jxqVt35eI3lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CombineHate=np.mean([textHate,emojiHate])\n",
        "print(CombineHate)\n",
        "\n",
        "combineNon=np.mean([textNon,emojiNon])\n",
        "print(combineNon)\n",
        "\n",
        "combineMid=np.mean([textHate,emojiNon])\n",
        "print(combineMid)"
      ],
      "metadata": {
        "id": "Qo2oA17w7rkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_emoji_vectors = np.array([get_emoji_vector(emoji) for emoji in X_actual_train_emoji])\n",
        "X_test_emoji_vectors = np.array([get_emoji_vector(emoji) for emoji in X_test_emoji])\n",
        "X_val_emoji_vectors=np.array([get_emoji_vector(emoji) for emoji in x_val_emoji])"
      ],
      "metadata": {
        "id": "LE9XS6JEra5N"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bGK30DrFlrwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combined the emoji and comment vectors"
      ],
      "metadata": {
        "id": "y7knzRnJrjx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vectors=np.mean([X_train_text_vectors,X_train_emoji_vectors],axis=0)\n",
        "\n",
        "X_test_vectors=np.mean([X_test_emoji_vectors,X_test_text_vectors],axis=0)\n",
        "\n",
        "X_val_vectors=np.mean([x_text_val_vectors,X_val_emoji_vectors],axis=0)"
      ],
      "metadata": {
        "id": "0OexuR3Frjc7"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model nature and aspects"
      ],
      "metadata": {
        "id": "h0amIYIbr4tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True)\n"
      ],
      "metadata": {
        "id": "IeuMnzCEr4Z5"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset convertion into the one hot encoding"
      ],
      "metadata": {
        "id": "DvXD1CBdFk3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "num_classes = 3\n",
        "y_actual_train_encoded = to_categorical(y_actual_train, num_classes=num_classes)\n",
        "y_val_encoded = to_categorical(y_val, num_classes=num_classes)\n",
        "y_test_encoded=to_categorical(y_test,num_classes=num_classes)"
      ],
      "metadata": {
        "id": "W85e1ObnAqOb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the model"
      ],
      "metadata": {
        "id": "JtaDlekp-Kw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Feed forword neural network"
      ],
      "metadata": {
        "id": "nRW7LzoVHYhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "\n",
        "input_dim = 300\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512,activation='relu',input_dim=input_dim))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "metadata": {
        "id": "4PnUipbrHKyG",
        "outputId": "1325da4f-dc54-42b1-bacd-24f27e8d3e51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oEQVUSUVJFvq"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_vectors, y_actual_train_encoded, epochs=100, batch_size=64, validation_data=(X_val_vectors, y_val_encoded), callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "Z3U6pdOuiVNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd111f4-323e-49e3-f79f-ea431d1c60d0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.4284 - loss: 0.9399 - val_accuracy: 0.7943 - val_loss: 0.5036\n",
            "Epoch 2/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7922 - loss: 0.5225 - val_accuracy: 0.7936 - val_loss: 0.4829\n",
            "Epoch 3/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8043 - loss: 0.5085 - val_accuracy: 0.7943 - val_loss: 0.4919\n",
            "Epoch 4/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8031 - loss: 0.4952 - val_accuracy: 0.7929 - val_loss: 0.4807\n",
            "Epoch 5/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8134 - loss: 0.4763 - val_accuracy: 0.7929 - val_loss: 0.4771\n",
            "Epoch 6/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8177 - loss: 0.4559 - val_accuracy: 0.7943 - val_loss: 0.4580\n",
            "Epoch 7/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8229 - loss: 0.4458 - val_accuracy: 0.8346 - val_loss: 0.4316\n",
            "Epoch 8/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8287 - loss: 0.4345 - val_accuracy: 0.8298 - val_loss: 0.4171\n",
            "Epoch 9/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8425 - loss: 0.4060 - val_accuracy: 0.8407 - val_loss: 0.4006\n",
            "Epoch 10/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8335 - loss: 0.4118 - val_accuracy: 0.8353 - val_loss: 0.4075\n",
            "Epoch 11/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8450 - loss: 0.3961 - val_accuracy: 0.8401 - val_loss: 0.3948\n",
            "Epoch 12/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8522 - loss: 0.3855 - val_accuracy: 0.8442 - val_loss: 0.3914\n",
            "Epoch 13/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8589 - loss: 0.3671 - val_accuracy: 0.8455 - val_loss: 0.3854\n",
            "Epoch 14/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8599 - loss: 0.3615 - val_accuracy: 0.8462 - val_loss: 0.3848\n",
            "Epoch 15/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8614 - loss: 0.3638 - val_accuracy: 0.8421 - val_loss: 0.3945\n",
            "Epoch 16/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8534 - loss: 0.3785 - val_accuracy: 0.8428 - val_loss: 0.3973\n",
            "Epoch 17/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8543 - loss: 0.3639 - val_accuracy: 0.8462 - val_loss: 0.3930\n",
            "Epoch 18/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8664 - loss: 0.3590 - val_accuracy: 0.8414 - val_loss: 0.3839\n",
            "Epoch 19/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8523 - loss: 0.3764 - val_accuracy: 0.8360 - val_loss: 0.4319\n",
            "Epoch 20/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8653 - loss: 0.3486 - val_accuracy: 0.8503 - val_loss: 0.3893\n",
            "Epoch 21/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8702 - loss: 0.3375 - val_accuracy: 0.8503 - val_loss: 0.3824\n",
            "Epoch 22/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8765 - loss: 0.3420 - val_accuracy: 0.8510 - val_loss: 0.3820\n",
            "Epoch 23/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8666 - loss: 0.3550 - val_accuracy: 0.8448 - val_loss: 0.3963\n",
            "Epoch 24/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8596 - loss: 0.3573 - val_accuracy: 0.8401 - val_loss: 0.4036\n",
            "Epoch 25/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8747 - loss: 0.3464 - val_accuracy: 0.8483 - val_loss: 0.3796\n",
            "Epoch 26/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8682 - loss: 0.3885 - val_accuracy: 0.8496 - val_loss: 0.3754\n",
            "Epoch 27/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8739 - loss: 0.3435 - val_accuracy: 0.8524 - val_loss: 0.3722\n",
            "Epoch 28/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8784 - loss: 0.3298 - val_accuracy: 0.8585 - val_loss: 0.3711\n",
            "Epoch 29/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8797 - loss: 0.3286 - val_accuracy: 0.8469 - val_loss: 0.3829\n",
            "Epoch 30/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8761 - loss: 0.3355 - val_accuracy: 0.8462 - val_loss: 0.3837\n",
            "Epoch 31/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8741 - loss: 0.3398 - val_accuracy: 0.8496 - val_loss: 0.3794\n",
            "Epoch 32/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8762 - loss: 0.3337 - val_accuracy: 0.8530 - val_loss: 0.3764\n",
            "Epoch 33/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8755 - loss: 0.3307 - val_accuracy: 0.8489 - val_loss: 0.4005\n",
            "Epoch 34/100\n",
            "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8689 - loss: 0.3356 - val_accuracy: 0.8530 - val_loss: 0.4024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model\n",
        "\n",
        "prediction=model.predict(X_test_vectors)\n",
        "loss, accuracy = model.evaluate(X_test_vectors, y_test_encoded)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "mhZON-9eO1Fj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4d496c-d7ae-4cc1-c692-0a0d25799068"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m58/58\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m58/58\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8554 - loss: 0.3459\n",
            "Test Loss: 0.3524697721004486, Test Accuracy: 0.8605795502662659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classification report for feedforward neural network"
      ],
      "metadata": {
        "id": "Tx1WTxObcZDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "predicted_classes = np.array(prediction)\n",
        "prediction=prediction.argmax(axis=1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, prediction))"
      ],
      "metadata": {
        "id": "Z6LNPt1McX7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbebf39-7dbb-4be0-8c7a-2c4aac19ecf2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.91      0.83       610\n",
            "           1       0.87      0.82      0.84       610\n",
            "           2       1.00      0.85      0.92       609\n",
            "\n",
            "    accuracy                           0.86      1829\n",
            "   macro avg       0.87      0.86      0.86      1829\n",
            "weighted avg       0.87      0.86      0.86      1829\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test by manually"
      ],
      "metadata": {
        "id": "SlkXkWC_2zJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "user_comment = input(\"Enter a comment: \")\n",
        "\n",
        "\n",
        "emoji_pattern = re.compile(r'\\p{So}')\n",
        "\n",
        "emojis = emoji_pattern.findall(user_comment)\n",
        "\n",
        "text_without_emojis = emoji_pattern.sub('', user_comment)\n",
        "\n",
        "user_text_vector = get_sentence_vector(user_comment)\n",
        "user_emoji_vector=get_emoji_vector(emojis[0])\n",
        "\n",
        "user_comment_vector=np.mean([user_text_vector,user_emoji_vector],axis=0)\n",
        "\n",
        "user_comment_vector = user_comment_vector.reshape(1,-1)\n",
        "\n",
        "prediction = knn.predict(user_comment_vector)\n",
        "\n",
        "print(f\"Predicted Class: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qUNR9iQ2y9q",
        "outputId": "c3ac566b-b595-4253-d618-884d92c94db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a comment: à·ƒà¶‚à·„à·’à¶¯à·’à¶ºà·à·€à¶§ à¶…à·€à·”à¶½à¶šà·Š à·€à·™à¶ºà·’ à¶¯ğŸ˜‚\n",
            "Predicted Class: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "vFtFl-54Kqzh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}