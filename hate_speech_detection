{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0+B6VhU6ETLOzb0MnwLN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/senaldm/Research_Project/blob/main/hate_speech_detection\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install necessary packages and libraries"
      ],
      "metadata": {
        "id": "YjMOr-WNkVfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "id": "yGZU10yMkVH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419ddb15-c728-45f5-9585-a86d235e548e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.4/68.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4227136 sha256=50f089cea8091606e93bd9e28517826f7b32cc7ea03724a4e20679577bd5d153\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.12.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary packages and libraries"
      ],
      "metadata": {
        "id": "w6jxjaWMj9m6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWNqkY4Lj6Yp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten, Dropout,Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import gensim.models as gsm\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import LSTM\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the dataset"
      ],
      "metadata": {
        "id": "HbPcjWFsmIN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "dataset_file = '/content/drive/MyDrive/new_updated_dataset.csv'\n",
        "\n",
        "data=pd.read_csv(dataset_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUDQSk3Sl3_l",
        "outputId": "56c0e1cf-c221-4599-de40-72f567cf539c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import pre-trained models(fasttext and emoji2Vec)"
      ],
      "metadata": {
        "id": "7NkOi8colfzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import fastText model for text\n",
        "\n",
        "fastText_model_path = '/content/drive/MyDrive/cc.si.300.bin/cc.si.300.bin'\n",
        "fasttext.FastText.eprint = lambda x: None\n",
        "ft = fasttext.load_model(fastText_model_path)\n",
        "\n",
        "\n",
        "#import emoji2Vec model for emojis\n",
        "\n",
        "e2v = gsm.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/emoji2vec.bin', binary=True)"
      ],
      "metadata": {
        "id": "jYyHFU0YkNC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle the dataset"
      ],
      "metadata": {
        "id": "rINgHTJcm9i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = shuffle(data)\n"
      ],
      "metadata": {
        "id": "ciWCT9Qlm-H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset"
      ],
      "metadata": {
        "id": "x5kkIL4WoLkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combined the emoji and text column for split in same points\n",
        "\n",
        "combined_data = pd.concat([data['comment'], data['expression_emoji']], axis=1)\n",
        "\n",
        "#split the dataset into train, test, validation\n",
        "combined_train, combined_test, y_train, y_test = train_test_split(combined_data, data['label'], test_size=0.2, random_state=42, stratify=data['label'])\n",
        "\n",
        "combined_actual_train,combined_val,y_actual_train,y_val=train_test_split(combined_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "#devide each column under train, test, validation\n",
        "X_actual_train_text, X_actual_train_emoji = combined_actual_train['comment'], combined_actual_train['expression_emoji']\n",
        "\n",
        "X_test_text, X_test_emoji= combined_test['comment'], combined_test['expression_emoji']\n",
        "\n",
        "x_val_text,x_val_emoji=combined_val['comment'],combined_val['expression_emoji']\n"
      ],
      "metadata": {
        "id": "bFvIKiC7FWT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform word to vector using fastText model for comment column"
      ],
      "metadata": {
        "id": "6_hLp25GnR8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_vector(word):\n",
        "    try:\n",
        "        wordVec=ft.get_word_vector(word)\n",
        "\n",
        "\n",
        "        return wordVec\n",
        "    except KeyError:\n",
        "        return np.zeros(ft.get_dimension())\n",
        "\n",
        "\n",
        "\n",
        "def get_sentence_vector(sentence):\n",
        "\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "    vectors = [get_word_vector(word) for word in sentence.split()]\n",
        "\n",
        "    return np.mean(vectors, axis=0)"
      ],
      "metadata": {
        "id": "Pwl7cVLmnTUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_text_vectors=np.array([get_sentence_vector(sentence) for sentence in X_actual_train_text])\n",
        "X_test_text_vectors=np.array([get_sentence_vector(sentence) for sentence in X_test_text])\n",
        "x_text_val_vectors=np.array([get_sentence_vector(sentence) for sentence in x_val_text])"
      ],
      "metadata": {
        "id": "jSKIuYT5pbx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " perform Emoji to vector task using emoji2Vec model for expression_emoji column"
      ],
      "metadata": {
        "id": "Ctj3GEOFrTN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(get_word_vector('‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eIGQsmyrYmw",
        "outputId": "932f41b4-4c7d-4169-8e18-eb23973d319f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009083676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(get_sentence_vector('‡∂∏‡∑ö‡∂ö‡∑ô‡∂≠‡∑ä ‡∑Ä‡∑ô‡∂±‡∂∏‡∂∏ ‡∂Ü‡∂≠‡∂Ω‡∑ä ‡∂ë‡∂ö‡∂ö‡∑ä ‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è!!')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv_2bCnCzMKI",
        "outputId": "4332203e-c9ab-4c5f-9308-3dbf50c122ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00010161365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_emoji_vector(emoji):\n",
        "\n",
        "    try:\n",
        "        value=-e2v[emoji]\n",
        "\n",
        "        return value\n",
        "\n",
        "    except KeyError:\n",
        "        return np.zeros(300)"
      ],
      "metadata": {
        "id": "I-Ut5-b_rUTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textHate=np.mean(get_word_vector('‡∂¥‡∑ú‡∂±‡∑ä‡∂±‡∂∫‡∑ô‡∂ö‡∑ä‡∂Ø'))\n",
        "print(textHate)\n",
        "emojiHate=np.mean(get_emoji_vector('üò°'))\n",
        "print(emojiHate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_beAu6A4H-A",
        "outputId": "0bdff9f0-9a5f-43c9-d833-921db0560623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0007260342\n",
            "-0.0015154603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textNon=np.mean(get_word_vector('‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è'))\n",
        "print(textNon)\n",
        "emojiNon=np.mean(get_emoji_vector('üòÇ'))\n",
        "print(emojiNon)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxLae8NB6rmv",
        "outputId": "24b96a34-3355-4168-a48a-49ca8d0ff007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009083676\n",
            "0.0021346046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textNon=np.mean(get_word_vector('‡∂≠‡∑í‡∂∫‡∑ô‡∂±‡∑Ä‡∑è'))\n",
        "print(textNon)\n",
        "emojiNon=np.mean(get_emoji_vector('ü•∞'))\n",
        "print(emojiNon)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxqVt35eI3lP",
        "outputId": "720840d2-af08-44b5-bd3f-b8311e97773d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009083676\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CombineHate=np.mean([textHate,emojiHate])\n",
        "print(CombineHate)\n",
        "\n",
        "combineNon=np.mean([textNon,emojiNon])\n",
        "print(combineNon)\n",
        "\n",
        "combineMid=np.mean([textHate,emojiNon])\n",
        "print(combineMid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo2oA17w7rkZ",
        "outputId": "a8216252-300d-4af4-e449-b66080f8efe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0011207473\n",
            "0.0004541838134173304\n",
            "-0.0003630171122495085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_emoji_vectors = np.array([get_emoji_vector(emoji) for emoji in X_actual_train_emoji])\n",
        "X_test_emoji_vectors = np.array([get_emoji_vector(emoji) for emoji in X_test_emoji])\n",
        "X_val_emoji_vectors=np.array([get_emoji_vector(emoji) for emoji in x_val_emoji])"
      ],
      "metadata": {
        "id": "LE9XS6JEra5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bGK30DrFlrwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combined the emoji and comment vectors"
      ],
      "metadata": {
        "id": "y7knzRnJrjx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vectors=np.mean([X_train_text_vectors,X_train_emoji_vectors],axis=0)\n",
        "\n",
        "X_test_vectors=np.mean([X_test_emoji_vectors,X_test_text_vectors],axis=0)\n",
        "\n",
        "X_val_vectors=np.mean([x_text_val_vectors,X_val_emoji_vectors],axis=0)"
      ],
      "metadata": {
        "id": "0OexuR3Frjc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model nature and aspects"
      ],
      "metadata": {
        "id": "h0amIYIbr4tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True)\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train),y= y_train)\n",
        "\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "IeuMnzCEr4Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert target classes into onehot encoding\n",
        "num_classes = 3\n",
        "y_actual_train_encoded = to_categorical(y_actual_train, num_classes=num_classes)\n",
        "y_val_encoded = to_categorical(y_val, num_classes=num_classes)\n",
        "y_test_encoded=to_categorical(y_test,num_classes=num_classes)"
      ],
      "metadata": {
        "id": "W85e1ObnAqOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the model"
      ],
      "metadata": {
        "id": "JtaDlekp-Kw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Feed forword neural network"
      ],
      "metadata": {
        "id": "nRW7LzoVHYhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 300\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512,activation='relu',input_dim=input_dim))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "metadata": {
        "id": "4PnUipbrHKyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oEQVUSUVJFvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train_vectors, y_actual_train_encoded, epochs=100, batch_size=64, validation_data=(X_val_vectors, y_val_encoded), callbacks=[early_stopping], class_weight=class_weight_dict)\n"
      ],
      "metadata": {
        "id": "Z3U6pdOuiVNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model\n",
        "\n",
        "prediction=model.predict(X_test_vectors)\n",
        "loss, accuracy = model.evaluate(X_test_vectors, y_test_encoded)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhZON-9eO1Fj",
        "outputId": "080eaf35-eea3-497e-d9aa-26b7d95e6458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58/58 [==============================] - 0s 2ms/step\n",
            "58/58 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8742\n",
            "Test Loss: 0.33739298582077026, Test Accuracy: 0.874248206615448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classification report for feedforward neural network"
      ],
      "metadata": {
        "id": "Tx1WTxObcZDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "predicted_classes = np.array(prediction)\n",
        "prediction=prediction.argmax(axis=1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6LNPt1McX7X",
        "outputId": "452486cd-e83f-45b8-83da-72c36a3efae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.92      0.84       610\n",
            "           1       0.90      0.85      0.87       610\n",
            "           2       1.00      0.85      0.92       609\n",
            "\n",
            "    accuracy                           0.87      1829\n",
            "   macro avg       0.89      0.87      0.88      1829\n",
            "weighted avg       0.89      0.87      0.88      1829\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using LSTM model"
      ],
      "metadata": {
        "id": "XIPzixvpH0FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_dim = 300\n",
        "\n",
        "modelLSTM = Sequential()\n",
        "timesteps=1\n",
        "\n",
        "modelLSTM.add(Bidirectional(LSTM(128, return_sequences=True,input_shape=(timesteps, input_dim))))\n",
        "modelLSTM.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "\n",
        "modelLSTM.add(Dense(num_classes, activation='softmax'))"
      ],
      "metadata": {
        "id": "vat_yF2Iu4m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the vectors\n",
        "X_train_vecotrs_reshaped = X_train_vectors.reshape(X_train_vectors.shape[0],timesteps, X_train_vectors.shape[1])\n",
        "X_val_vectors_reshaped = X_val_vectors.reshape(X_val_vectors.shape[0], timesteps, X_val_vectors.shape[1])\n",
        "X_test_vecotrs_reshaped = X_test_vectors.reshape(X_test_vectors.shape[0], timesteps, X_test_vectors.shape[1])"
      ],
      "metadata": {
        "id": "AlEsqo1BDKkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "modelLSTM.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'],run_eagerly=True)"
      ],
      "metadata": {
        "id": "qOXBK4TmOQjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(y_actual_train_encoded.shape)\n",
        "# print(X_train_vecotrs_reshaped.shape)\n",
        "# # Remove the extra dimension from the target data\n",
        "y_actual_train_encoded_reshaped  = y_actual_train_encoded.reshape(y_actual_train_encoded.shape[0],1, num_classes)\n",
        "y_val_encoded_reshaped=y_val_encoded.reshape(y_val_encoded.shape[0],1,num_classes)\n",
        "y_actual_train_encoded_reshaped = y_actual_train_encoded_reshaped.reshape(y_actual_train_encoded_reshaped.shape)\n",
        "y_val_encoded_reshaped = y_val_encoded_reshaped.reshape(y_val_encoded_reshaped.shape)\n",
        "y_test_encoded_reshaped=y_test_encoded_reshaped.reshape(y_test_encoded_reshaped.shape)\n",
        "# y_test_encoded_reshaped=y_test_encoded.reshape(y_test_encoded.shape[0],1,num_classes)\n",
        "# y_actual_train_encoded_reshaped = y_actual_train_encoded_reshaped.reshape(-1, 1, 3)\n",
        "# print(y_actual_train_encoded_reshaped.shape)\n",
        "# # y_actual_train_encoded_reshaped = y_actual_train_encoded_reshaped.reshape(-1, 1, 3)\n",
        "print(y_actual_train_encoded_reshaped.shape)\n",
        "print(y_val_encoded_reshaped.shape)\n",
        "print(X_train_vecotrs_reshaped.shape)\n",
        "print(X_val_vectors_reshaped.shape)\n",
        "# print(modelLSTM.output_shape)"
      ],
      "metadata": {
        "id": "CVnsjTd588Cp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd9f70e-c87c-40b4-ddfe-164de4ef356a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5851, 1, 3)\n",
            "(1463, 1, 3)\n",
            "(5851, 1, 300)\n",
            "(1463, 1, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual_train=np.array(y_actual_train)\n",
        "print(X_train_vectors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPjhvHhL-aRJ",
        "outputId": "e83be10e-2838-42fc-9db4-c6f1e5c09e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5851, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelLSTM.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "61n_eFtw_fld",
        "outputId": "1a87317f-a12d-433c-e1da-8e1f96ed36cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-96d9cbddd1d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3504\u001b[0m         \"\"\"\n\u001b[1;32m   3505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3506\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3507\u001b[0m                 \u001b[0;34m\"This model has not yet been built. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3508\u001b[0m                 \u001b[0;34m\"Build the model first by calling `build()` or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# y_actual_train_encoded_reshaped = y_actual_train_encoded_reshaped.squeeze()\n",
        "# y_val_encoded_reshaped = y_val_encoded_reshaped.squeeze()\n",
        "# modelLSTM.fit(X_train_vecotrs_reshaped.reshape(-1, 1, 300), y_actual_train_encoded_reshaped.reshape(-1, 1, 3), epochs=100, batch_size=64, validation_data=(X_val_vectors_reshaped.reshape(-1, 1, 300), y_val_encoded_reshaped.reshape(-1, 1, 3)), callbacks=[early_stopping])\n",
        "# modelLSTM.fit(X_train_vecotrs_reshaped, y_actual_train_encoded_reshaped.reshape(-1, 1, 3), epochs=100, batch_size=64, validation_data=(y_val_encoded_reshaped, y_val_encoded), callbacks=[early_stopping])\n",
        "# Train the model\n",
        "modelLSTM.fit(X_train_vecotrs_reshaped, y_actual_train_encoded_reshaped, epochs=100, batch_size=64, validation_data=(X_val_vectors_reshaped, y_val_encoded_reshaped), callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qLOPJSbOc3a",
        "outputId": "d86c6755-0ed2-401e-cc26-87b12f0cec3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "92/92 [==============================] - 17s 162ms/step - loss: 0.7233 - accuracy: 0.7378 - val_loss: 0.4743 - val_accuracy: 0.7990\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - 13s 139ms/step - loss: 0.4337 - accuracy: 0.8156 - val_loss: 0.4420 - val_accuracy: 0.8442\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - 13s 144ms/step - loss: 0.4036 - accuracy: 0.8337 - val_loss: 0.3961 - val_accuracy: 0.8387\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - 13s 143ms/step - loss: 0.3832 - accuracy: 0.8428 - val_loss: 0.3842 - val_accuracy: 0.8448\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - 13s 143ms/step - loss: 0.3765 - accuracy: 0.8453 - val_loss: 0.3857 - val_accuracy: 0.8592\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - 13s 144ms/step - loss: 0.3653 - accuracy: 0.8523 - val_loss: 0.3827 - val_accuracy: 0.8530\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - 13s 138ms/step - loss: 0.3611 - accuracy: 0.8542 - val_loss: 0.3685 - val_accuracy: 0.8606\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - 14s 149ms/step - loss: 0.3562 - accuracy: 0.8563 - val_loss: 0.3784 - val_accuracy: 0.8524\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - 75s 150ms/step - loss: 0.3549 - accuracy: 0.8593 - val_loss: 0.3705 - val_accuracy: 0.8544\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - 14s 148ms/step - loss: 0.3527 - accuracy: 0.8571 - val_loss: 0.3676 - val_accuracy: 0.8578\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - 13s 146ms/step - loss: 0.3520 - accuracy: 0.8599 - val_loss: 0.3992 - val_accuracy: 0.8448\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - 12s 132ms/step - loss: 0.3468 - accuracy: 0.8609 - val_loss: 0.3641 - val_accuracy: 0.8647\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - 12s 134ms/step - loss: 0.3405 - accuracy: 0.8638 - val_loss: 0.3644 - val_accuracy: 0.8612\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - 13s 138ms/step - loss: 0.3359 - accuracy: 0.8670 - val_loss: 0.3883 - val_accuracy: 0.8462\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - 13s 141ms/step - loss: 0.3425 - accuracy: 0.8646 - val_loss: 0.3707 - val_accuracy: 0.8612\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - 13s 141ms/step - loss: 0.3375 - accuracy: 0.8638 - val_loss: 0.3959 - val_accuracy: 0.8305\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - 13s 144ms/step - loss: 0.3342 - accuracy: 0.8641 - val_loss: 0.3673 - val_accuracy: 0.8530\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - 13s 144ms/step - loss: 0.3368 - accuracy: 0.8658 - val_loss: 0.3846 - val_accuracy: 0.8612\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x780c0f6b4070>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "T1Q1ySOHHVGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test LSTM model\n",
        "\n",
        "prediction=modelLSTM.predict(X_test_vecotrs_reshaped)\n",
        "\n",
        "loss, accuracy = modelLSTM.evaluate(X_test_vecotrs_reshaped, y_test_encoded_reshaped)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK8fpx25VsNP",
        "outputId": "42c5a5c7-cc86-43ae-d3d9-78798f4bd072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58/58 [==============================] - 5s 81ms/step\n",
            "58/58 [==============================] - 5s 93ms/step - loss: 0.3740 - accuracy: 0.8469\n",
            "Test Loss: 0.37399184703826904, Test Accuracy: 0.8469108939170837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification report for LSTM"
      ],
      "metadata": {
        "id": "opQTNU0jV1AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "predicted_classes = np.array(prediction)\n",
        "prediction=prediction.argmax(axis=1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "McVLaQeaV0vH",
        "outputId": "0ac52aa5-dea0-467a-db64-140f5535d452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Singleton array 0 cannot be considered a valid collection.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-5883c824e326>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classification Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \"\"\"\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2310\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0;34m\"Singleton array %r cannot be considered a valid collection.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             )\n",
            "\u001b[0;31mTypeError\u001b[0m: Singleton array 0 cannot be considered a valid collection."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vecotrs_reshaped = X_train_vectors.reshape(X_train_vectors.shape[0],X_train_vectors.shape[1])\n",
        "X_val_vectors_reshaped = X_val_vectors.reshape(X_val_vectors.shape[0], 1,X_val_vectors.shape[1])\n",
        "X_test_vecotrs_reshaped = X_test_vectors.reshape(X_test_vectors.shape[0],1, X_test_vectors.shape[1])"
      ],
      "metadata": {
        "id": "drA8hDo-V8FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try with KMeans (Optional)"
      ],
      "metadata": {
        "id": "zD-fA3zOkrmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "\n",
        "kmeans.fit(X_train_vectors)\n",
        "\n",
        "predictWithKmeans = kmeans.predict(X_test_vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjQQDG51kjmC",
        "outputId": "0eb9f08b-205e-4589-8ef6-5ed6f2d3513d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictWithKmeans)\n",
        "print(\"Accuracy of K-means on test data:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96icpoOOlKA6",
        "outputId": "98ff9972-a1a1-443b-d284-8bb517afb92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of K-means on test data: 0.13504647348277748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "predicted_classes = np.array(predictWithKmeans)\n",
        "\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predicted_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z_LJFa6mMLy",
        "outputId": "3f626765-61e7-4582-abc8-ec7d26764cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       610\n",
            "           1       0.23      0.40      0.30       610\n",
            "           2       0.00      0.00      0.00       609\n",
            "\n",
            "    accuracy                           0.14      1829\n",
            "   macro avg       0.08      0.13      0.10      1829\n",
            "weighted avg       0.08      0.14      0.10      1829\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try with K nearest Neighbours(Optional)"
      ],
      "metadata": {
        "id": "5aAbDTgCmTdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "knn.fit(X_train_vectors, y_actual_train)\n",
        "\n",
        "predictWithKNN = knn.predict(X_test_vectors)"
      ],
      "metadata": {
        "id": "sE6YaXHWmhGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictWithKNN)\n",
        "print(\"Accuracy of KNN on test data:\", accuracy)\n",
        "print(y_test)\n",
        "print(predictWithKNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y1uwk5bmwtD",
        "outputId": "a8ca278f-39ec-4163-9cdd-abdf316b3b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN on test data: 0.8305084745762712\n",
            "[1 1 2 ... 2 1 1]\n",
            "[0 1 2 ... 2 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "predicted_classes = np.array(predictWithKNN)\n",
        "print(predicted_classes)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predicted_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDirb4-7m5NG",
        "outputId": "aec49daa-12cd-44d8-c6a3-1d557d430559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 ... 2 1 1]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.76      0.77       610\n",
            "           1       0.75      0.89      0.82       610\n",
            "           2       0.98      0.84      0.91       609\n",
            "\n",
            "    accuracy                           0.83      1829\n",
            "   macro avg       0.84      0.83      0.83      1829\n",
            "weighted avg       0.84      0.83      0.83      1829\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test by manually"
      ],
      "metadata": {
        "id": "SlkXkWC_2zJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "user_comment = input(\"Enter a comment: \")\n",
        "\n",
        "\n",
        "emoji_pattern = re.compile(r'\\p{So}')\n",
        "\n",
        "emojis = emoji_pattern.findall(user_comment)\n",
        "\n",
        "text_without_emojis = emoji_pattern.sub('', user_comment)\n",
        "\n",
        "user_text_vector = get_sentence_vector(user_comment)\n",
        "user_emoji_vector=get_emoji_vector(emojis[0])\n",
        "\n",
        "user_comment_vector=np.mean([user_text_vector,user_emoji_vector],axis=0)\n",
        "\n",
        "user_comment_vector = user_comment_vector.reshape(1,-1)\n",
        "\n",
        "prediction = knn.predict(user_comment_vector)\n",
        "\n",
        "print(f\"Predicted Class: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qUNR9iQ2y9q",
        "outputId": "c3ac566b-b595-4253-d618-884d92c94db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a comment: ‡∑É‡∂Ç‡∑Ñ‡∑í‡∂Ø‡∑í‡∂∫‡∑è‡∑Ä‡∂ß ‡∂Ö‡∑Ä‡∑î‡∂Ω‡∂ö‡∑ä ‡∑Ä‡∑ô‡∂∫‡∑í ‡∂ØüòÇ\n",
            "Predicted Class: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "vFtFl-54Kqzh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}